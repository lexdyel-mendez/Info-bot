{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (open(r'Dataset/dataset.txt',encoding='utf-8').read())\n",
    "text = text.lower()\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = dict((c,i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 11070\n",
      "Total Vocab: 57\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total characters:\",n_chars)\n",
    "print(\"Total Vocab:\",n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 10970\n"
     ]
    }
   ],
   "source": [
    "#Prepare the dataset of input to output pairs of encoded as integers\n",
    "seq_lenght = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0,n_chars-seq_lenght, 1):\n",
    "    seq_in = text[i:i+seq_lenght]\n",
    "    seq_out = text[i + seq_lenght]\n",
    "    dataX.append([char_to_int[char]for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns:\",n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x to be [samples, times steps, features]\n",
    "X = numpy.reshape(dataX,(n_patterns, seq_lenght,1))\n",
    "# Normalize\n",
    "X = X/float(n_vocab)\n",
    "# One-Hot Encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256,input_shape = (X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer = \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10970/10970 [==============================] - 19s 2ms/step - loss: 3.1555\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.15546, saving model to weights-improvement-01-3.1555.hdf5\n",
      "Epoch 2/20\n",
      "10970/10970 [==============================] - 16s 1ms/step - loss: 3.0357\n",
      "\n",
      "Epoch 00002: loss improved from 3.15546 to 3.03566, saving model to weights-improvement-02-3.0357.hdf5\n",
      "Epoch 3/20\n",
      "10970/10970 [==============================] - 16s 1ms/step - loss: 3.0300\n",
      "\n",
      "Epoch 00003: loss improved from 3.03566 to 3.03001, saving model to weights-improvement-03-3.0300.hdf5\n",
      "Epoch 4/20\n",
      "10970/10970 [==============================] - 16s 1ms/step - loss: 3.0233\n",
      "\n",
      "Epoch 00004: loss improved from 3.03001 to 3.02331, saving model to weights-improvement-04-3.0233.hdf5\n",
      "Epoch 5/20\n",
      "10970/10970 [==============================] - 17s 2ms/step - loss: 3.0172\n",
      "\n",
      "Epoch 00005: loss improved from 3.02331 to 3.01719, saving model to weights-improvement-05-3.0172.hdf5\n",
      "Epoch 6/20\n",
      "10970/10970 [==============================] - 16s 1ms/step - loss: 3.0105\n",
      "\n",
      "Epoch 00006: loss improved from 3.01719 to 3.01053, saving model to weights-improvement-06-3.0105.hdf5\n",
      "Epoch 7/20\n",
      "10970/10970 [==============================] - 16s 1ms/step - loss: 3.0084\n",
      "\n",
      "Epoch 00007: loss improved from 3.01053 to 3.00842, saving model to weights-improvement-07-3.0084.hdf5\n",
      "Epoch 8/20\n",
      "10970/10970 [==============================] - 16s 1ms/step - loss: 2.9954\n",
      "\n",
      "Epoch 00008: loss improved from 3.00842 to 2.99543, saving model to weights-improvement-08-2.9954.hdf5\n",
      "Epoch 9/20\n",
      "10970/10970 [==============================] - 16s 1ms/step - loss: 2.9668\n",
      "\n",
      "Epoch 00009: loss improved from 2.99543 to 2.96677, saving model to weights-improvement-09-2.9668.hdf5\n",
      "Epoch 10/20\n",
      "10970/10970 [==============================] - 16s 1ms/step - loss: 2.9387\n",
      "\n",
      "Epoch 00010: loss improved from 2.96677 to 2.93875, saving model to weights-improvement-10-2.9387.hdf5\n",
      "Epoch 11/20\n",
      "10970/10970 [==============================] - 16s 1ms/step - loss: 2.9191\n",
      "\n",
      "Epoch 00011: loss improved from 2.93875 to 2.91906, saving model to weights-improvement-11-2.9191.hdf5\n",
      "Epoch 12/20\n",
      "10970/10970 [==============================] - ETA: 0s - loss: 2.906 - 17s 2ms/step - loss: 2.9068\n",
      "\n",
      "Epoch 00012: loss improved from 2.91906 to 2.90684, saving model to weights-improvement-12-2.9068.hdf5\n",
      "Epoch 13/20\n",
      "10970/10970 [==============================] - 21s 2ms/step - loss: 2.8937\n",
      "\n",
      "Epoch 00013: loss improved from 2.90684 to 2.89366, saving model to weights-improvement-13-2.8937.hdf5\n",
      "Epoch 14/20\n",
      "10970/10970 [==============================] - 18s 2ms/step - loss: 2.8806\n",
      "\n",
      "Epoch 00014: loss improved from 2.89366 to 2.88061, saving model to weights-improvement-14-2.8806.hdf5\n",
      "Epoch 15/20\n",
      "10970/10970 [==============================] - 17s 2ms/step - loss: 2.8659\n",
      "\n",
      "Epoch 00015: loss improved from 2.88061 to 2.86588, saving model to weights-improvement-15-2.8659.hdf5\n",
      "Epoch 16/20\n",
      "10970/10970 [==============================] - 16s 1ms/step - loss: 2.8478\n",
      "\n",
      "Epoch 00016: loss improved from 2.86588 to 2.84783, saving model to weights-improvement-16-2.8478.hdf5\n",
      "Epoch 17/20\n",
      "10970/10970 [==============================] - 16s 1ms/step - loss: 2.8392\n",
      "\n",
      "Epoch 00017: loss improved from 2.84783 to 2.83917, saving model to weights-improvement-17-2.8392.hdf5\n",
      "Epoch 18/20\n",
      "10970/10970 [==============================] - 17s 2ms/step - loss: 2.8296\n",
      "\n",
      "Epoch 00018: loss improved from 2.83917 to 2.82961, saving model to weights-improvement-18-2.8296.hdf5\n",
      "Epoch 19/20\n",
      "10970/10970 [==============================] - 16s 1ms/step - loss: 2.8170\n",
      "\n",
      "Epoch 00019: loss improved from 2.82961 to 2.81699, saving model to weights-improvement-19-2.8170.hdf5\n",
      "Epoch 20/20\n",
      "10970/10970 [==============================] - 16s 1ms/step - loss: 2.8089\n",
      "\n",
      "Epoch 00020: loss improved from 2.81699 to 2.80894, saving model to weights-improvement-20-2.8089.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14b27d25a90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,epochs=20, batch_size=128,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights \n",
    "filename = \"weights-improvement-20-2.8089.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy',optimizer = 'adam')\n",
    "\n",
    "int_to_char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" eloped by them after hurricane maria on energy resilience. their work is a reflection of their effor \"\n",
      " te tee tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee aat  aoe tee a"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
